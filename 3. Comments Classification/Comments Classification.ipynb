{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9730f52",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Comments classification for an online shop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eea48e2",
   "metadata": {},
   "source": [
    "The online shop's website enables users to leave comments on products as well as on other users' comments. To maintain a positive user experience, the shop needs to identify negative (toxic/insulting) comments and flag them for manual moderation.\n",
    "\n",
    "**Our goal**: to detect negative (toxic/insulting) comments.\n",
    "\n",
    "To achieve this goal, we will develop a machine learning model. The customer's requirement for the quality metric is an F1 score of at least 0.75.\n",
    "\n",
    "**Data overview:** the dataset contains 159 292 entries, consisting of text data (`text`) and target label (`toxic`).\n",
    "\n",
    "**Research plan**:\n",
    "1. Data loading and preprocessing.\n",
    "2. Model creation.\n",
    "3. Conclusion."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929c9ec8",
   "metadata": {},
   "source": [
    "## 1. Data loading and preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0c5c8d",
   "metadata": {},
   "source": [
    "First of all we import all necessary libraries and dictionaries for natural language processing task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a07f0446",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\annad\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\annad\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\annad\\AppData\\Roaming\\nltk_data...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import re\n",
    "from IPython.display import display\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "\n",
    "STOP_WORDS = list(stopwords.words('english'))\n",
    "\n",
    "RANDOM_STATE = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "223b9fdd",
   "metadata": {},
   "source": [
    "Now we load the dataset and view the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4fa5cd37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  toxic\n",
       "0  Explanation\\nWhy the edits made under my usern...      0\n",
       "1  D'aww! He matches this background colour I'm s...      0\n",
       "2  Hey man, I'm really not trying to edit war. It...      0\n",
       "3  \"\\nMore\\nI can't make any real suggestions on ...      0\n",
       "4  You, sir, are my hero. Any chance you remember...      0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 159292 entries, 0 to 159450\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count   Dtype \n",
      "---  ------  --------------   ----- \n",
      " 0   text    159292 non-null  object\n",
      " 1   toxic   159292 non-null  int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 3.6+ MB\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('https://code.s3.yandex.net/datasets/toxic_comments.csv', index_col=[0])\n",
    "display(data.head())\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5c22c2",
   "metadata": {},
   "source": [
    "Let's look at the possible values in the `toxic` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49da20d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "toxic\n",
       "0    143106\n",
       "1     16186\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['toxic'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb01753",
   "metadata": {},
   "source": [
    "The feature takes only two unique values, indicating that we are solving binary classification problem. Additionally, we observe class imbalance, with ten times fewer examples of toxic comments.\n",
    "\n",
    "Since generating embeddings using BERT requires significant computational resources, we will not utilize the entire dataset. Instead, we will use a sample of 10000 messages, taking 5000 samples from each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d1f9584",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A block ohhhhhhhhhhhhhh noooooooooooo I'm sooo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20% reminds me of the percentage of German you...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Relation to independence \\n\\nThis concept of a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Milage\\nIt is more than 30 Miles from Liverpoo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Also don't seek revenge that user is an admin....</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  toxic\n",
       "0  A block ohhhhhhhhhhhhhh noooooooooooo I'm sooo...      1\n",
       "1  20% reminds me of the percentage of German you...      0\n",
       "2  Relation to independence \\n\\nThis concept of a...      0\n",
       "3  Milage\\nIt is more than 30 Miles from Liverpoo...      0\n",
       "4  Also don't seek revenge that user is an admin....      0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create subsets of each class\n",
    "class_0_subset = data[data['toxic'] == 0]\n",
    "class_1_subset = data[data['toxic'] == 1]\n",
    "\n",
    "# Take 5000 samples of each class\n",
    "sampled_class_0 = class_0_subset.sample(n=5000, random_state=RANDOM_STATE)\n",
    "sampled_class_1 = class_1_subset.sample(n=5000, random_state=RANDOM_STATE)\n",
    "\n",
    "# Combine them to create a new dataset and shuffle\n",
    "data_sample = pd.concat([sampled_class_0, sampled_class_1])\n",
    "data_sample = data_sample.sample(frac=1, random_state=RANDOM_STATE).reset_index(drop=True)\n",
    "data_sample.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37259389",
   "metadata": {},
   "source": [
    "Let's preprocess the text as follows:\n",
    "\n",
    "* Convert to lowercase,\n",
    "* Remove punctuation and digits,\n",
    "* Tokenize the text,\n",
    "* Lemmatize each token,\n",
    "* Remove stop words,\n",
    "* Reassemble the processed tokens back into a string separated by spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "150e7694",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for text preprocessing\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-zA-Z]', ' ', text)\n",
    "    \n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    \n",
    "    tokens = [token for token in tokens if token not in STOP_WORDS]\n",
    "    \n",
    "    preprocessed_text = ' '.join(tokens)\n",
    "    \n",
    "    return preprocessed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c8b84f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>block ohhhhhhhhhhhhhh noooooooooooo soooo like...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>reminds percentage german youth fighting battl...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>relation independence concept set generator se...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>milage mile liverpool blackpool like mile also...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>also seek revenge user admin talk</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  toxic\n",
       "0  block ohhhhhhhhhhhhhh noooooooooooo soooo like...      1\n",
       "1  reminds percentage german youth fighting battl...      0\n",
       "2  relation independence concept set generator se...      0\n",
       "3  milage mile liverpool blackpool like mile also...      0\n",
       "4                  also seek revenge user admin talk      0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply text preprocessing funcion to 'text' column\n",
    "data_sample['text'] = data_sample['text'].apply(lambda x: preprocess_text(x))\n",
    "data_sample.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a01c2f40",
   "metadata": {},
   "source": [
    "Using a pretrained BERT model, we will generate embeddings that will serve as input features for training our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0e73d0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device choice (GPU/CPU)\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "# Function for embedding creation\n",
    "def get_embeddings(data, tokenizer, model, max_len=512, batch_size=1):\n",
    "    tokenized = data['text'].apply(lambda x: tokenizer.encode(x, \n",
    "                                                              add_special_tokens=True, \n",
    "                                                              max_length=max_len,\n",
    "                                                              truncation=True))\n",
    "    \n",
    "    padded = np.array([i + [0]*(max_len - len(i)) for i in tokenized.values])\n",
    "    attention_mask = np.where(padded != 0, 1, 0)\n",
    "    \n",
    "    embeddings = []\n",
    "    for i in tqdm(range(padded.shape[0] // batch_size)):\n",
    "        batch = torch.LongTensor(padded[batch_size*i:batch_size*(i+1)]).to(device)\n",
    "        attention_mask_batch = torch.LongTensor(attention_mask[batch_size*i:batch_size*(i+1)]).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            batch_embeddings = model(batch, attention_mask=attention_mask_batch)\n",
    "        \n",
    "        embeddings.append(batch_embeddings[0][:,0,:].cpu().numpy())\n",
    "        del batch\n",
    "        del attention_mask_batch\n",
    "        del batch_embeddings\n",
    "        \n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1c6b1865",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [02:33<00:00, 65.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding creation took: 159.25 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Create embeddings\n",
    "start = time.time()\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased', output_hidden_states=True)\n",
    "model = model.to(device)\n",
    "\n",
    "text_embeddings = get_embeddings(data_sample, tokenizer, model, max_len=512, batch_size=1)\n",
    "\n",
    "# Create features from embeddings\n",
    "features = np.concatenate(text_embeddings)\n",
    "\n",
    "emb_time = round(time.time() - start, 2)\n",
    "print('Embedding creation took:', emb_time, 'seconds.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63568995",
   "metadata": {},
   "source": [
    "Data preprocessing is complete, now we can train our model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b732aaf",
   "metadata": {},
   "source": [
    "## 2. Model creation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afca6aaf",
   "metadata": {},
   "source": [
    "We will split the dataset into training and testing sets, ensuring that the target feature is proportionally represented in both sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "47335969",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainin set size: (7500, 768)\n",
      "Testing set size: (2500, 768)\n"
     ]
    }
   ],
   "source": [
    "# Train/test split\n",
    "X = features\n",
    "y = data_sample['toxic']\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    random_state=RANDOM_STATE,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "print('Trainin set size:', X_train.shape)\n",
    "print('Testing set size:', X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2ed30f",
   "metadata": {},
   "source": [
    "Now we will find the best hyperparameters for gradient boosting model (XGboost) using cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1e3e9f4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The search took: 138.75 seconds.\n",
      "\n",
      "\u001b[1mCross-validation results:\u001b[0m\n",
      "Model hyperparameters:\n",
      " XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.35, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=42, ...) \n",
      "\n",
      "F1-score: 0.842\n"
     ]
    }
   ],
   "source": [
    "# Parameters grid\n",
    "param_grid = {\n",
    "    'learning_rate': [0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5]\n",
    "}\n",
    "\n",
    "# Start cross-validation\n",
    "start = time.time()\n",
    "\n",
    "model_search = GridSearchCV(\n",
    "    estimator=XGBClassifier(objective='binary:logistic',\n",
    "                            random_state=RANDOM_STATE),\n",
    "    param_grid=param_grid,\n",
    "    cv=5,\n",
    "    scoring='f1',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "model_search.fit(X_train, y_train)\n",
    "\n",
    "search_time = round(time.time() - start, 2)\n",
    "print('The search took:', search_time, 'seconds.\\n')\n",
    "print('\\033[1m' + 'Cross-validation results:' + '\\033[0m')\n",
    "print('Model hyperparameters:\\n', model_search.best_estimator_,'\\n')\n",
    "print('F1-score:', (model_search.best_score_).round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c7ba01",
   "metadata": {},
   "source": [
    "F1-score during cross-validation meet the criteria. Let's test our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "08487a95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mTesting results:\u001b[0m\n",
      "F1-score: 0.858\n"
     ]
    }
   ],
   "source": [
    "print('\\033[1m' + 'Testing results:' + '\\033[0m')\n",
    "y_pred_test = model_search.predict(X_test)\n",
    "f1_score_test = f1_score(y_test, y_pred_test).round(3)\n",
    "print('F1-score:', f1_score_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a4d70c7",
   "metadata": {},
   "source": [
    "The model meets curtomer's requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbea1d6b",
   "metadata": {},
   "source": [
    "## 3. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5738fa9d",
   "metadata": {},
   "source": [
    "The goal of this project was to classify comments as neutral or negative (toxic/insulting).\n",
    "\n",
    "To achieve this goal, we built a machine learning model based on gradient boosting using the XGBoost library. The F1 score was used as the quality metric.\n",
    "\n",
    "The dataset contained 159 292 entries. The data showed a significant imbalance in the target feature.\n",
    "\n",
    "We preprocessed the text as follows:\n",
    "* Convert to lowercase,\n",
    "* Remove punctuation and digits,\n",
    "* Tokenize the text,\n",
    "* Lemmatize each token,\n",
    "* Remove stop words,\n",
    "* Reassemble the processed tokens back into a string separated by spaces.\n",
    "\n",
    "Next, using the BERT model, we generated embeddings and converted the texts to vector form.\n",
    "\n",
    "As a result of cross-validation, we determined that the best model has the following properties:\n",
    "\n",
    "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
    "              colsample_bylevel=None, colsample_bynode=None,\n",
    "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
    "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
    "              gamma=None, grow_policy=None, importance_type=None,\n",
    "              interaction_constraints=None, learning_rate=0.35, max_bin=None,\n",
    "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
    "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
    "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
    "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
    "              num_parallel_tree=None, random_state=42) \n",
    "\n",
    "Performance Metrics:\n",
    "* F1-score during cross-validation: 0.842\n",
    "* F1-score on the test set: 0.858\n",
    "\n",
    "The required quality level on the test set is an F1 score of at least 0.75. Therefore, the presented model meets the customer's requirements and can be used for comment classification on the website."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
